{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "91f9581e-1e6c-4b24-b3b3-1d9d10552e5d",
      "cell_type": "markdown",
      "source": "## A function that calculates the error between expected word frequency and article word frequency using the KL Divergence Method\n",
      "metadata": {}
    },
    {
      "id": "bba3233b-38cc-43a5-8328-9fccfd2ae73f",
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy \nimport wordfreq",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "<class 'ModuleNotFoundError'>",
          "evalue": "No module named 'wordfreq'",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m \n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwordfreq\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'wordfreq'"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 5
    },
    {
      "id": "c5ccaaab-2784-461b-a9fc-f30ac0c53a24",
      "cell_type": "code",
      "source": "class Analyser:\n\n    #Here assume that words is a loaded dict of structure: {\"total\": #number#, \"list\": {#all words with numbers#}}\n    @staticmethod\n    def calculate_probablity_values(words):\n        count = words[\"total\"]\n        words_df = pd.Series(words[\"list\"]).to_frame(\"count\")\n        words_df = words_df.sort_values(by='count',ascending=False)\n        words_df['count'] /= count\n        words_df.columns = [\"Article Frequency\"]\n        return words_df\n        \n    @staticmethod\n    def top_n_words_probability(n, language):\n        all_words = wordfreq.top_n_list(language, n)\n        probability_values = [wordfreq.word_frequency(w, language) for w in all_words]\n        lang_df = pd.DataFrame(data={\"Language Frequency\": probability_values}, index=pd.Index(all_words, name='word'))\n        return lang_df\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "id": "4bfbc463-3282-42e4-ae04-3b16ed9a8073",
      "cell_type": "code",
      "source": "def lang_confidence_score(word_counts, language_words_with_frequency):\n    words_df = Analyser.calculate_probablity_values(word_counts)\n    words_df.columns = [\"Data Frequency\"]\n    combined_df = words_df.join(language_words_with_frequency, how='right')\n    combined_df[\"Data Frequency\"] = combined_df[\"Data Frequency\"].fillna(1e-10)\n    combined_df[\"Data Frequency\"] = combined_df[\"Data Frequency\"] / combined_df[\"Data Frequency\"].sum()\n    combined_df[\"Language Frequency\"] = combined_df[\"Language Frequency\"] / combined_df[\"Language Frequency\"].sum()\n    \n    # KL Divergence Calculation\n    score = numpy.sum(combined_df[\"Language Frequency\"] * numpy.log(\n         combined_df[\"Language Frequency\"]/  \n         combined_df[\"Data Frequency\"]))\n\n    return score\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "id": "6a1c24f9-034e-426b-b13c-579f4ae11ac2",
      "cell_type": "markdown",
      "source": "# Experimental Results\n\nThe choice of the language had measurable yet not overwhelming impact on the outcome - predictably English won with lowest average error score - followed by German and with French being the least similar to expected frequency. All languages showed significantly better prediction of frequency for literally works than for wiki articles. Interestingly, the choice of the wiki (pokemon wiki for german and french and xkcd for english) had very little impact on prediction rates. It is postulated that as long as the articles cluster around a single range of topics such as science, japanese culture or sports, then the results will be quite similar for every type of content - with slight variation according to the amount of internet data in this topic. For the purpose of testing performance on literaly works \"Uncle Tom's Cabin\", \"Les Miserables\" and \"Also Sprach Zaratustra\" were chosen for their time period similarity and a wealth of vocabulary. Interestingly the chosen metric (KL divergence) showed increasing error rates as the number of matched words increased, finally reaching a maximum value after about 1000 words - in which case it was however still possible to spot what language the article was composed in. The score differences were maximised for K = 100. All six charts with the details of used articles are in the /charts folder  ",
      "metadata": {}
    },
    {
      "id": "21f51dc8-f59e-4751-a335-d00bd0c8091f",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}